{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VR8_yILVsoJWl-WZvI48eBv4hW0UkDjU","timestamp":1724220037231}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 0. 세팅"],"metadata":{"id":"oUaUvxFCMzH7"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfmPicRQlIfd","executionInfo":{"status":"ok","timestamp":1724217176148,"user_tz":-540,"elapsed":22113,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}},"outputId":"ca040fa4-8766-4968-9d12-98499a0cf95b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","model_number = 2\n","learning_rate = 0.0002\n","gamma = 0.98\n","game_round = 10\n","strategies = [\"Copycat\", \"All Cooperate\", \"All Cheat\", \"Grudger\", \"Detective\",\n","              \"Copykitten\", \"Simpleton\", \"Random\", \"Cheat-Downing\", \"Cooperate-Downing\",\n","              \"Joss\", \"Cheat-Tester\", \"Cooperate-Tester\", \"Tranquilizer\", \"Gradual\",\n","              \"Prober\", \"Pavlov\", \"Mistrust\", \"Per-Kind\", \"Per-Nasty\"]"],"metadata":{"id":"SyJcjuqo7KIH","executionInfo":{"status":"ok","timestamp":1724221132368,"user_tz":-540,"elapsed":487,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# 1. State, Action, Reward에 관한 환경 설정"],"metadata":{"id":"VmeMR-g8OeSC"}},{"cell_type":"markdown","source":["## 1-1. REINFORCE 알고리즘"],"metadata":{"id":"JXHgXYoDM6W-"}},{"cell_type":"code","source":["class REINFORCE(nn.Module):\n","    def __init__(self):\n","        super(REINFORCE, self).__init__()\n","        self.data = []\n","\n","        self.fc1 = nn.Linear(game_round * 2, 128)\n","        self.fc2 = nn.Linear(128, 2)\n","        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.softmax(self.fc2(x), dim=0)\n","        return x\n","\n","    def put_data(self, item):\n","        self.data.append(item)\n","\n","    def train_net(self):\n","        R = 0\n","        policy_loss = []\n","        self.optimizer.zero_grad()\n","        for r, prob in self.data[::-1]:\n","            R = r + gamma * R\n","            loss = -torch.log(prob) * R\n","            policy_loss.append(loss.unsqueeze(0))\n","        policy_loss = torch.cat(policy_loss).sum()\n","        policy_loss.backward()\n","        self.optimizer.step()\n","        self.data = []"],"metadata":{"id":"6yDZOLL-7OFP","executionInfo":{"status":"ok","timestamp":1724221140034,"user_tz":-540,"elapsed":5,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["## 1-2. 점수 획득 규칙"],"metadata":{"id":"3lefR_VKNB1j"}},{"cell_type":"code","source":["def reward(my_action, opponent_action):\n","    if [my_action, opponent_action] == [1, 1]: # 협력, 협력\n","        return 2\n","    elif [my_action, opponent_action] == [1, 0]: # 협력, 배신\n","        return -1\n","    elif [my_action, opponent_action] == [0, 1]: # 배신, 협력\n","        return 3\n","    elif [my_action, opponent_action] == [0, 0]: # 배신, 배신\n","        return 0"],"metadata":{"id":"6UJQDSOyjO91","executionInfo":{"status":"ok","timestamp":1724221140034,"user_tz":-540,"elapsed":3,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## 1-3. 상대방의 전략과 현재 게임 상태에 기반하여 상대방의 행동 선택"],"metadata":{"id":"7P2T8Y32NIgL"}},{"cell_type":"code","source":["class game():\n","    def reset():\n","        return [-1 for _ in range(game_round * 2)], strategies[random.randint(0, len(strategies)-1)]\n","\n","    def step(state, strategy, my_action):\n","        my_state = []\n","        opp_state = []\n","        for i, s in enumerate(state):\n","            if s == -1:\n","                cur_game_round = i // 2\n","                break\n","            if i % 2 == 0:\n","                my_state.append(s)\n","            else:\n","                opp_state.append(s)\n","\n","        if strategy == \"Copycat\":\n","            if cur_game_round == 0:\n","                opp_action = 1\n","            else:\n","                prev_my_action = my_state[-1]\n","                opp_action = prev_my_action\n","\n","        elif strategy == \"All Cooperate\":\n","            opp_action = 1\n","\n","        elif strategy == \"All Cheat\":\n","            opp_action = 0\n","\n","        elif strategy == \"Grudger\":\n","            if cur_game_round == 0:\n","                opp_action = 1\n","            else:\n","                opp_action = 1 if 0 not in my_state else 0\n","\n","        elif strategy == \"Detective\":\n","            if cur_game_round in [0, 2, 3]:\n","                opp_action = 1\n","            elif cur_game_round == 1:\n","                opp_action = 0\n","            else:\n","                if 0 in my_state[:4]:\n","                    prev_my_action = my_state[-1]\n","                    opp_action = prev_my_action\n","                else:\n","                    opp_action = 0\n","\n","        elif strategy == \"Copykitten\":\n","            if cur_game_round in [0, 1]:\n","                opp_action = 1\n","            else:\n","                prev_prev_my_action, prev_my_action = my_state[-2], my_state[-1]\n","                opp_action = 0 if [prev_prev_my_action, prev_my_action] == [0, 0] else 1\n","\n","        elif strategy == \"Simpleton\":\n","            if cur_game_round == 0:\n","                opp_action = 1\n","            else:\n","                prev_my_action, prev_opp_action = my_state[-1], opp_state[-1]\n","                if prev_my_action == 0:\n","                    if prev_opp_action == 0:\n","                        opp_action = 1\n","                    else:\n","                        opp_action = 0\n","                else:\n","                    opp_action = prev_opp_action\n","\n","        elif strategy == \"Random\":\n","            opp_action = random.randint(0, 1)\n","\n","        elif strategy == \"Cheat-Downing\":\n","            my_coop, my_cheat = 0, 0\n","            for ms in my_state:\n","                if ms == 0:\n","                    my_cheat += 1\n","                else:\n","                    my_coop += 1\n","            if my_cheat >= my_coop:\n","                opp_action = 0\n","            else:\n","                opp_action = 1\n","\n","        elif strategy == \"Cooperate-Downing\":\n","            my_coop, my_cheat = 0, 0\n","            for ms in my_state:\n","                if ms == 0:\n","                    my_cheat += 1\n","                else:\n","                    my_coop += 1\n","            if my_cheat > my_coop:\n","                opp_action = 0\n","            else:\n","                opp_action = 1\n","\n","        elif strategy == \"Joss\":\n","            if cur_game_round == 0:\n","                prob = random.randint(0, 9)\n","                if prob == 0:\n","                    opp_action = 0\n","                else:\n","                  opp_action = 1\n","            else:\n","                prev_my_action = my_state[-1]\n","                opp_action = prev_my_action\n","\n","        elif strategy == \"Cheat-Tester\":\n","            if cur_game_round < game_round // 2:\n","                opp_action = random.randint(0, 1)\n","            else:\n","                my_coop, my_cheat = 0, 0\n","                for i in range(game_round // 2):\n","                    if my_state[i] == 0:\n","                        my_cheat += 1\n","                    else:\n","                        my_coop += 1\n","                if my_cheat >= my_coop:\n","                    opp_action = 0\n","                else:\n","                    opp_action = 1\n","\n","        elif strategy == \"Cooperate-Tester\":\n","            if cur_game_round < game_round // 2:\n","                opp_action = random.randint(0, 1)\n","            else:\n","                my_coop, my_cheat = 0, 0\n","                for i in range(game_round // 2):\n","                    if my_state[i] == 0:\n","                        my_cheat += 1\n","                    else:\n","                        my_coop += 1\n","                if my_cheat > my_coop:\n","                    opp_action = 0\n","                else:\n","                    opp_action = 1\n","\n","        elif strategy == \"Tranquilizer\":\n","            opp_coop, opp_cheat = 0, 0\n","            for os in opp_state:\n","                if os == 0:\n","                    opp_cheat += 1\n","                else:\n","                    opp_coop += 1\n","            if (opp_cheat + 1) / (opp_coop + opp_cheat + 1) < 0.25:\n","                opp_action = 0\n","            else:\n","                opp_action = 1\n","\n","        elif strategy == \"Gradual\":\n","            if cur_game_round == 0:\n","                  opp_action = 1\n","            else:\n","                my_cheat = 0\n","                for ms in my_state:\n","                    if ms == 0:\n","                        my_cheat += 1\n","                x, cheat_sum = 1, 0\n","                for mc in range(my_cheat):\n","                    cheat_sum += x\n","                    x += 1\n","                opp_cheat = 0\n","                for os in opp_state:\n","                    if os == 0:\n","                        opp_cheat += 1\n","                if cheat_sum > opp_cheat:\n","                    opp_action = 0\n","                else:\n","                    opp_action = 1\n","\n","        elif strategy == \"Prober\":\n","            if cur_game_round in [0, 1, 2]:\n","                if cur_game_round == 0:\n","                    opp_action = 1\n","                else:\n","                    opp_action = 0\n","            else:\n","                prev_three_round = [3, 4, 5]\n","                while True:\n","                    if cur_game_round in prev_three_round:\n","                        for i in range(3):\n","                            prev_three_round[i] -= 3\n","                        break\n","                    else:\n","                        for i in range(3):\n","                            prev_three_round[i] += 3\n","                if my_state[prev_three_round[1]] == 1 and my_state[prev_three_round[2]] == 0:\n","                    opp_action = my_state[-1]\n","                else:\n","                    if cur_game_round % 3 == 0:\n","                        opp_action = 0\n","                    else:\n","                        opp_action = 1\n","\n","        elif strategy == \"Pavlov\":\n","            if cur_game_round == 0:\n","                opp_action = 1\n","            else:\n","                if opp_state[-1] == my_state[-1]:\n","                    opp_action = 1\n","                else:\n","                    opp_action = 0\n","\n","        elif strategy == \"Mistrust\":\n","            if cur_game_round == 0:\n","                opp_action = 0\n","            else:\n","                prev_my_action = my_state[-1]\n","                opp_action = prev_my_action\n","\n","        elif strategy == \"Per-Kind\":\n","            if cur_game_round % 3 in [0, 1]:\n","                opp_action = 1\n","            else:\n","                opp_action = 0\n","\n","        elif strategy == \"Per-Nasty\":\n","            if cur_game_round % 3 == 0:\n","                opp_action = 1\n","            else:\n","                opp_action = 0\n","\n","        state[cur_game_round * 2] = my_action\n","        state[cur_game_round * 2 + 1] = opp_action\n","\n","        return state, reward(my_action, opp_action), (-1 not in state)"],"metadata":{"id":"E3FrqkeL8TEW","executionInfo":{"status":"ok","timestamp":1724221140035,"user_tz":-540,"elapsed":4,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["class selfplay:\n","    def reset():\n","        return [-1 for _ in range(game_round * 2)], [-1 for _ in range(game_round * 2)]\n","\n","    def step(state_i, state_j, i_action, j_action):\n","        i_state_by_i = []\n","        j_state_by_i = []\n","        i_state_by_j = []\n","        j_state_by_j = []\n","        for i, s in enumerate(state_i):\n","            if s == -1:\n","                cur_game_round = i // 2\n","                break\n","            if i % 2 == 0:\n","                i_state_by_i.append(s)\n","            else:\n","                j_state_by_i.append(s)\n","        for i, s in enumerate(state_j):\n","            if s == -1:\n","                cur_game_round = i // 2\n","                break\n","            if i % 2 == 0:\n","                i_state_by_j.append(s)\n","            else:\n","                j_state_by_j.append(s)\n","        state_i[cur_game_round * 2] = i_action\n","        state_i[cur_game_round * 2 + 1] = j_action\n","        state_j[cur_game_round * 2] = j_action\n","        state_j[cur_game_round * 2 + 1] = i_action\n","\n","        return state_i, state_j, reward(i_action, j_action), reward(j_action, i_action), (-1 not in state_i)\n"],"metadata":{"id":"Rqyzkl3gbGqN","executionInfo":{"status":"ok","timestamp":1724221140035,"user_tz":-540,"elapsed":3,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# 2. 강화학습"],"metadata":{"id":"T1TSLrUxNjpZ"}},{"cell_type":"code","source":["models = [REINFORCE() for _ in range(model_number)]\n","for model in models:\n","  score = 0.0\n","  print_interval = 1000\n","\n","  X = []\n","  Y = []\n","  for n_epi in range(70001):\n","      s, opp_st = game.reset()\n","      done = False\n","\n","      while not done:\n","\n","          prob = model(torch.tensor(s).float())\n","          m = Categorical(prob)\n","          a = m.sample()\n","          s_prime, r, done = game.step(s, opp_st, a)\n","          model.put_data((r,prob[a]))\n","          s = s_prime\n","          score += r\n","\n","      model.train_net()\n","\n","      if n_epi%print_interval==0 and n_epi!=0:\n","          print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n","          X.append(n_epi)\n","          Y.append(score/print_interval)\n","          score = 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnnnc0U7nfRy","executionInfo":{"status":"ok","timestamp":1724222660876,"user_tz":-540,"elapsed":968258,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}},"outputId":"5d8f2459-45df-45d6-da82-981ec20ecc15"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["# of episode :1000, avg score : 12.578\n","# of episode :2000, avg score : 13.575\n","# of episode :3000, avg score : 13.799\n","# of episode :4000, avg score : 13.538\n","# of episode :5000, avg score : 14.285\n","# of episode :6000, avg score : 13.892\n","# of episode :7000, avg score : 14.1\n","# of episode :8000, avg score : 14.645\n","# of episode :9000, avg score : 14.25\n","# of episode :10000, avg score : 14.066\n","# of episode :11000, avg score : 14.056\n","# of episode :12000, avg score : 14.574\n","# of episode :13000, avg score : 14.746\n","# of episode :14000, avg score : 14.567\n","# of episode :15000, avg score : 14.477\n","# of episode :16000, avg score : 14.109\n","# of episode :17000, avg score : 14.353\n","# of episode :18000, avg score : 14.244\n","# of episode :19000, avg score : 14.501\n","# of episode :20000, avg score : 14.433\n","# of episode :21000, avg score : 14.473\n","# of episode :22000, avg score : 14.446\n","# of episode :23000, avg score : 14.265\n","# of episode :24000, avg score : 14.844\n","# of episode :25000, avg score : 14.745\n","# of episode :26000, avg score : 14.984\n","# of episode :27000, avg score : 14.296\n","# of episode :28000, avg score : 14.665\n","# of episode :29000, avg score : 14.507\n","# of episode :30000, avg score : 14.489\n","# of episode :31000, avg score : 15.229\n","# of episode :32000, avg score : 14.819\n","# of episode :33000, avg score : 14.832\n","# of episode :34000, avg score : 14.996\n","# of episode :35000, avg score : 15.101\n","# of episode :36000, avg score : 15.015\n","# of episode :37000, avg score : 14.845\n","# of episode :38000, avg score : 15.232\n","# of episode :39000, avg score : 15.305\n","# of episode :40000, avg score : 15.137\n","# of episode :41000, avg score : 15.338\n","# of episode :42000, avg score : 15.244\n","# of episode :43000, avg score : 15.32\n","# of episode :44000, avg score : 15.326\n","# of episode :45000, avg score : 15.277\n","# of episode :46000, avg score : 15.913\n","# of episode :47000, avg score : 15.092\n","# of episode :48000, avg score : 15.482\n","# of episode :49000, avg score : 15.722\n","# of episode :50000, avg score : 15.696\n","# of episode :51000, avg score : 15.661\n","# of episode :52000, avg score : 15.732\n","# of episode :53000, avg score : 15.332\n","# of episode :54000, avg score : 15.724\n","# of episode :55000, avg score : 16.015\n","# of episode :56000, avg score : 15.644\n","# of episode :57000, avg score : 16.053\n","# of episode :58000, avg score : 15.662\n","# of episode :59000, avg score : 15.836\n","# of episode :60000, avg score : 16.303\n","# of episode :61000, avg score : 15.973\n","# of episode :62000, avg score : 15.71\n","# of episode :63000, avg score : 16.124\n","# of episode :64000, avg score : 16.078\n","# of episode :65000, avg score : 16.373\n","# of episode :66000, avg score : 16.468\n","# of episode :67000, avg score : 15.992\n","# of episode :68000, avg score : 16.132\n","# of episode :69000, avg score : 16.055\n","# of episode :70000, avg score : 16.405\n","# of episode :1000, avg score : 12.345\n","# of episode :2000, avg score : 13.824\n","# of episode :3000, avg score : 13.498\n","# of episode :4000, avg score : 14.095\n","# of episode :5000, avg score : 13.901\n","# of episode :6000, avg score : 13.922\n","# of episode :7000, avg score : 14.087\n","# of episode :8000, avg score : 14.447\n","# of episode :9000, avg score : 13.868\n","# of episode :10000, avg score : 14.565\n","# of episode :11000, avg score : 14.474\n","# of episode :12000, avg score : 14.523\n","# of episode :13000, avg score : 14.437\n","# of episode :14000, avg score : 14.742\n","# of episode :15000, avg score : 14.091\n","# of episode :16000, avg score : 14.28\n","# of episode :17000, avg score : 14.386\n","# of episode :18000, avg score : 14.017\n","# of episode :19000, avg score : 14.176\n","# of episode :20000, avg score : 14.744\n","# of episode :21000, avg score : 13.952\n","# of episode :22000, avg score : 14.461\n","# of episode :23000, avg score : 14.621\n","# of episode :24000, avg score : 14.427\n","# of episode :25000, avg score : 14.519\n","# of episode :26000, avg score : 14.845\n","# of episode :27000, avg score : 14.787\n","# of episode :28000, avg score : 14.508\n","# of episode :29000, avg score : 14.894\n","# of episode :30000, avg score : 14.696\n","# of episode :31000, avg score : 14.969\n","# of episode :32000, avg score : 14.776\n","# of episode :33000, avg score : 14.581\n","# of episode :34000, avg score : 15.232\n","# of episode :35000, avg score : 14.787\n","# of episode :36000, avg score : 15.368\n","# of episode :37000, avg score : 15.676\n","# of episode :38000, avg score : 15.452\n","# of episode :39000, avg score : 15.605\n","# of episode :40000, avg score : 15.352\n","# of episode :41000, avg score : 15.313\n","# of episode :42000, avg score : 16.005\n","# of episode :43000, avg score : 15.65\n","# of episode :44000, avg score : 15.787\n","# of episode :45000, avg score : 15.353\n","# of episode :46000, avg score : 16.143\n","# of episode :47000, avg score : 15.957\n","# of episode :48000, avg score : 16.385\n","# of episode :49000, avg score : 16.131\n","# of episode :50000, avg score : 16.007\n","# of episode :51000, avg score : 16.169\n","# of episode :52000, avg score : 16.258\n","# of episode :53000, avg score : 16.02\n","# of episode :54000, avg score : 16.015\n","# of episode :55000, avg score : 15.776\n","# of episode :56000, avg score : 16.025\n","# of episode :57000, avg score : 15.965\n","# of episode :58000, avg score : 15.798\n","# of episode :59000, avg score : 15.9\n","# of episode :60000, avg score : 16.126\n","# of episode :61000, avg score : 16.365\n","# of episode :62000, avg score : 16.062\n","# of episode :63000, avg score : 16.273\n","# of episode :64000, avg score : 16.478\n","# of episode :65000, avg score : 16.652\n","# of episode :66000, avg score : 15.872\n","# of episode :67000, avg score : 16.033\n","# of episode :68000, avg score : 16.265\n","# of episode :69000, avg score : 16.229\n","# of episode :70000, avg score : 16.46\n"]}]},{"cell_type":"code","source":["def train_net(self):\n","    R = 0\n","    policy_loss = []\n","    self.optimizer.zero_grad()\n","    for r, prob in self.data[::-1]:\n","        R = r + gamma * R\n","        loss = -torch.log(prob) * R\n","        policy_loss.append(loss.unsqueeze(0))\n","    policy_loss = torch.cat(policy_loss).sum()\n","    policy_loss.backward()\n","    self.optimizer.step()\n","    self.data = []\n"],"metadata":{"id":"NToTDmcMxx2v","executionInfo":{"status":"ok","timestamp":1724224318462,"user_tz":-540,"elapsed":2,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["for _ in range(10000):\n","    for i in range(model_number - 1):\n","        for j in range(i + 1, model_number):\n","            score_i = 0.0\n","            score_j = 0.0\n","            epinumber = 100\n","\n","            listx = []\n","            listy = []\n","            for n_epi in range(epinumber):\n","                s_i, s_j = selfplay.reset()\n","                done = False\n","\n","                # Train model[i] against model[j]\n","                while not done:\n","                    prob_i = models[i](torch.tensor(s_i).float())\n","                    prob_j = models[j](torch.tensor(s_j).float())\n","                    m_i = Categorical(prob_i)\n","                    m_j = Categorical(prob_j)\n","                    a_i = m_i.sample()\n","                    a_j = m_j.sample()\n","                    s_i_prime, s_j_prime, reward_i, reward_j, done = selfplay.step(s_i, s_j, a_i, a_j)\n","                    models[i].put_data((reward_i, prob_i[a_i]))\n","                    models[j].put_data((reward_j, prob_j[a_j]))\n","                    s_i = s_i_prime\n","                    s_j = s_j_prime\n","                    score_i += reward_i\n","                    score_j += reward_j\n","\n","                models[i].train_net()  # Train model[i] after the episode\n","\n","                if n_epi == epinumber-1:\n","                    print(\"# of episode :{}, score_i : {}, score_j : {}\".format(n_epi, score_i / print_interval, score_j / print_interval))\n","                    listx.append(n_epi)\n","                    listy.append(score_i / print_interval)\n","                    score_i = 0.0\n","                    score_j = 0.0\n","\n","            # Now, train model[j] against model[i] as the fixed opponent\n","            for n_epi in range(epinumber):\n","                s_i, s_j = selfplay.reset()\n","                done = False\n","\n","                while not done:\n","                    prob_i = models[j](torch.tensor(s_j).float())\n","                    prob_j = models[i](torch.tensor(s_i).float())\n","                    m_i = Categorical(prob_i)\n","                    m_j = Categorical(prob_j)\n","                    a_i = m_i.sample()\n","                    a_j = m_j.sample()\n","                    s_i_prime, s_j_prime, reward_i, reward_j, done = selfplay.step(s_j, s_i, a_i, a_j)\n","                    models[j].put_data((reward_i, prob_i[a_i]))\n","                    models[i].put_data((reward_j, prob_j[a_j]))\n","                    s_i = s_i_prime\n","                    s_j = s_j_prime\n","                    score_i += reward_i\n","                    score_j += reward_j\n","\n","                models[j].train_net()  # Train model[j] after the episode\n","\n","                if n_epi == epinumber-1:\n","                    print(\"# of episode :{}, score_i : {}, score_j : {}\".format(n_epi, score_i / print_interval, score_j / print_interval))\n","                    listx.append(n_epi)\n","                    listy.append(score_i / print_interval)\n","                    score_i = 0.0\n","                    score_j = 0.0\n"],"metadata":{"id":"uF12I7IPRlWg","colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"status":"error","timestamp":1724224321765,"user_tz":-540,"elapsed":604,"user":{"displayName":"2106박광후","userId":"10031823912537686846"}},"outputId":"c8c58bf8-d2d9-418c-98de-c5148760a37e"},"execution_count":60,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-bb0c818e4a0e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mscore_j\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train model[i] after the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn_epi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mepinumber\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-57fb00318f62>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}]}]}